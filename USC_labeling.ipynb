{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzysearch import find_near_matches\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileToDf(location1,location2): \n",
    "    #read data into a useful dataframe\n",
    "    df1 = pd.read_table(location1, sep = ',')\n",
    "    df2 = pd.read_table(location2, sep = ',')\n",
    "    \n",
    "    cmt_lst = df1.values[:,5].tolist()\n",
    "    issid_lst = df1.values[:,1].tolist()\n",
    "    \n",
    "    lb_lst_uniq = df2.values[:,8].tolist()\n",
    "    issid_lst2 = df2.values[:,1].tolist()\n",
    "    \n",
    "    # merge lists with different lengths \n",
    "    d1 = dict(zip(issid_lst2,lb_lst_uniq))\n",
    "    lb_lst = []\n",
    "    for iss in issid_lst:\n",
    "        lb_lst.append(d1[iss])\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame({'Content': cmt_lst,\n",
    "                       'Issue_id': issid_lst,\n",
    "                       'Label': lb_lst\n",
    "                       })\n",
    "    #print(df.head())\n",
    "    df.shape\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=simple_url_re.match)\n",
    "\n",
    "def replaceUrls(dataframe):\n",
    "    #replace urls (not effective for irregular urls)\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "    strList = []\n",
    "\n",
    "    for cmt in dataframe['Content'].iteritems():\n",
    "        msg = str(cmt)\n",
    "\n",
    "        doc = nlp(msg)\n",
    "\n",
    "        for i, token in enumerate(doc):\n",
    "            if re.match(r'.*(http[s]:\\/\\/)stackoverflow.com.*', token.lemma_):\n",
    "                token.lemma_ = 'SO-url'\n",
    "            elif re.match(r'.*(http[s]:\\/\\/).*', token.lemma_):\n",
    "                token.lemma_ = 'other-url'\n",
    "\n",
    "        modifiedStr = ''\n",
    "        for token in doc:\n",
    "            modifiedStr += ' ' + token.lemma_\n",
    "        strList.append(modifiedStr)\n",
    "    dataframe['Content'] = strList\n",
    "    #print(dataframe.head())\n",
    "    dataframe.shape\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions to extract the sentences\n",
    "\n",
    "def findSE(input):\n",
    "    soStr = str(input)\n",
    "    nlst = []\n",
    "    if len(soStr) == 0:\n",
    "        return nlst\n",
    "    counter = 2\n",
    "    wrt = 0\n",
    "    str1 = \"\"\n",
    "    str2 = \"\"\n",
    "    while counter > 0:\n",
    "        for i in range(len(soStr)):\n",
    "            if soStr[i] == \",\":\n",
    "                wrt = 0\n",
    "                counter -= 1\n",
    "            if wrt == 1 and counter == 2:\n",
    "                str1 += soStr[i]\n",
    "            if wrt == 1 and counter == 1:\n",
    "                str2 += soStr[i]\n",
    "            if soStr[i] == \"=\":\n",
    "                wrt = 1\n",
    "    int1 = int(str1)\n",
    "    int2 = int(str2)\n",
    "    nlst.append(int1)\n",
    "    nlst.append(int2)\n",
    "    return nlst\n",
    "\n",
    "def findBefore(cmtStr, wSize, start):\n",
    "    counter = wSize\n",
    "    beforeList = []\n",
    "    beforeWord = \"\"\n",
    "    pointer = start - 1\n",
    "    while counter > 0:\n",
    "        if pointer == -1:\n",
    "            beforeList.insert(0, beforeWord)\n",
    "            return beforeList\n",
    "        if cmtStr[pointer] == \" \" and cmtStr[pointer-1] != \" \":\n",
    "            if beforeWord != \"\":\n",
    "                beforeList.insert(0, beforeWord)\n",
    "                counter -= 1\n",
    "            if counter == 0:\n",
    "                return beforeList\n",
    "            beforeWord = \"\"\n",
    "            pointer -= 1\n",
    "        elif cmtStr[pointer] == \" \" and cmtStr[pointer-1] == \" \":\n",
    "            pointer -= 1\n",
    "        else:\n",
    "            beforeWord = cmtStr[pointer] + beforeWord\n",
    "            pointer -= 1\n",
    "    return beforeList\n",
    "\n",
    "def findAfter(cmtStr, wSize, end):\n",
    "    counter = wSize\n",
    "    afterList = []\n",
    "    afterWord = \"\"\n",
    "    pointer = end + 1\n",
    "    while counter > 0:\n",
    "        if pointer == len(cmtStr):\n",
    "            afterList.append(afterWord)\n",
    "            return afterList\n",
    "        if cmtStr[pointer] == \" \" and cmtStr[pointer+1] != \" \":\n",
    "            if afterWord != \"\":\n",
    "                afterList.append(afterWord)\n",
    "                counter -= 1\n",
    "            if counter == 0:\n",
    "                return afterList\n",
    "            afterWord = \"\"\n",
    "            pointer += 1\n",
    "        elif cmtStr[pointer] == \" \" and cmtStr[pointer+1] == \" \":\n",
    "            pointer += 1\n",
    "        else:\n",
    "            afterWord += cmtStr[pointer]\n",
    "            pointer += 1\n",
    "    return afterList\n",
    "\n",
    "def findWindow(cmtStr, wSize, start, end, keyWord):\n",
    "    before = findBefore(cmtStr, wSize, start)\n",
    "    after = findAfter(cmtStr, wSize, end)\n",
    "    window = before + keyWord + after\n",
    "    newStr = ''\n",
    "    for i in window:\n",
    "        newStr += ' ' + i\n",
    "    return newStr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWindow(dataframe):\n",
    "    nlst = []\n",
    "    isslst = []\n",
    "\n",
    "    for cmt,isid in zip(dataframe['Content'],dataframe['Issue_id']):\n",
    "        matches1 = find_near_matches('stack overflow', str(cmt).lower(), max_l_dist=2)\n",
    "        matches2 = find_near_matches(' SO ', str(cmt), max_l_dist=0)\n",
    "        matches3 = find_near_matches('so question', str(cmt).lower(), max_l_dist=0)\n",
    "        matches4 = find_near_matches('so issue', str(cmt).lower(), max_l_dist=0)\n",
    "        matches5 = find_near_matches('so forum', str(cmt).lower(), max_l_dist=0)\n",
    "        #print(matches1)\n",
    "        #print(matches2)\n",
    "        #print(matches3)\n",
    "        #print(matches4)\n",
    "        #print(matches5)\n",
    "        strCmt = str(cmt)\n",
    "        if len(matches1) == 0 and len(matches2) == 0 and len(matches3) == 0 and len(matches4) == 0 and len(matches5) == 0:\n",
    "            #nlst.append([])\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(len(matches1)):\n",
    "                se = findSE(matches1[i])\n",
    "                start = se[0]\n",
    "                end = se[1]\n",
    "                #print(start)\n",
    "                #print(end)\n",
    "                #print(cmt)\n",
    "                currentWin = findWindow(strCmt, 5, start, end, ['stack overflow'])\n",
    "                #print(currentWin)\n",
    "                nlst.append(currentWin)\n",
    "                if se != []:\n",
    "                    isslst.append(isid)\n",
    "            for i in range(len(matches2)):\n",
    "                se = findSE(matches2[i])\n",
    "                start = se[0]\n",
    "                end = se[1]\n",
    "                #print(start)\n",
    "                #print(end)\n",
    "                #print(cmt)\n",
    "                currentWin = findWindow(strCmt, 5, start, end, ['SO'])\n",
    "                #print(currentWin)\n",
    "                nlst.append(currentWin)\n",
    "                if se != []:\n",
    "                    isslst.append(isid)\n",
    "            for i in range(len(matches3)):\n",
    "                se = findSE(matches3[i])\n",
    "                start = se[0]\n",
    "                end = se[1]\n",
    "                #print(start)\n",
    "                #print(end)\n",
    "                #print(cmt)\n",
    "                currentWin = findWindow(strCmt, 5, start, end, ['so question'])\n",
    "                #print(currentWin)\n",
    "                nlst.append(currentWin)\n",
    "                if se != []:\n",
    "                    isslst.append(isid)\n",
    "            for i in range(len(matches4)):\n",
    "                se = findSE(matches4[i])\n",
    "                start = se[0]\n",
    "                end = se[1]\n",
    "                #print(start)\n",
    "                #print(end)\n",
    "                #print(cmt)\n",
    "                currentWin = findWindow(strCmt, 5, start, end, ['so issue'])\n",
    "                #print(currentWin)\n",
    "                nlst.append(currentWin)\n",
    "                if se != []:\n",
    "                    isslst.append(isid)\n",
    "            for i in range(len(matches5)):\n",
    "                se = findSE(matches5[i])\n",
    "                start = se[0]\n",
    "                end = se[1]\n",
    "                #print(start)\n",
    "                #print(end)\n",
    "                #print(cmt)\n",
    "                currentWin = findWindow(strCmt, 5, start, end, ['so forum'])\n",
    "                #print(currentWin)\n",
    "                nlst.append(currentWin)\n",
    "                if se != []:\n",
    "                    isslst.append(isid)\n",
    "            #nlst.append(currentRow)\n",
    "            #currentRow = []\n",
    "    #print(nlst)\n",
    "    print(len(nlst))\n",
    "\n",
    "    return nlst,isslst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined(location1,location2):\n",
    "    dataframe = fileToDf(location1,location2)\n",
    "    dataframe = replaceUrls(dataframe)\n",
    "    sentenceLst,issid_lst = extractWindow(dataframe)\n",
    "    return sentenceLst,issid_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\CISC880_SO_Directed_Issues\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\CISC880_SO_Directed_Issues\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Content  Issue_id              Label\n",
      "0             31532  status: duplicate\n",
      "1             31532  status: duplicate\n",
      "2             31541    status: invalid\n",
      "3             31541    status: invalid\n",
      "4             31541    status: invalid\n"
     ]
    }
   ],
   "source": [
    "#enter all data locations\n",
    "locations = ['E:/qrf/Lab Stuff/newDataset/spring-boot_PRcomments.txt']\n",
    "locations_of_q = ['E:/qrf/Lab Stuff/newDataset/spring-boot_PRs.txt']\n",
    "allSentences = []\n",
    "allissueid = []\n",
    "for x,y in zip(locations,locations_of_q):\n",
    "    newsentencelst,newisslst = combined(x,y)\n",
    "    allSentences += newsentencelst\n",
    "    allissueid += newisslst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(allSentences, show_progress_bar=True)\n",
    "\n",
    "#Compute cosine-similarity between all pairs of sentences\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the similarity of a sentence with itself\n",
    "print(cosine_scores[0][0])\n",
    "\n",
    "# the similarity of a sentence with others\n",
    "print(cosine_scores[0][1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convers cosine similarity to distance, to be used later in the clustering algorithm\n",
    "distance = np.abs(1 - np.abs(cosine_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the distance of a sentence with itself\n",
    "print(distance[0][0])\n",
    "\n",
    "# the distance of a sentence with others\n",
    "print(distance[0][1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.5, min_samples=5, metric='precomputed').fit(distance)\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for label, count in Counter(labels).items():\n",
    "    print(label,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sentences = pd.DataFrame({'issueid':allissueid, 'sentence':allSentences, 'label':labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sentences.to_csv('spring-boot_labeled_sentences.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CISC880_SO_Directed_Issues",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d165d7b32b707b5ef0415ea55472466dce050ae69047a3fe51c46091313b5ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
